\chapter{Discussion}


\section{Reflection on the results}

Usability tests are import tools for the evaluation of a prototype. However, with a limited number of test persons, a quantitative analysis is not possible due to the necessity of a test group that is sufficiently large to produce statistically useful data. Even for a qualitative analysis, using the opinion of only one physical therapist may result in responses that are biased towards a certain view the therapist has on the subject. To distinguish between genuine design defects and problems only encountered due to individual bias, it is necessary to have a large enough test group. For the scope of this thesis, the number of user tests is limited, so a more careful approach is needed when interpreting the results of the test.\\

The test users, including the physical therapist, require time to get familiar with non-traditional gesture-based interface elements. The time needed depends on the person and his knowledge and experiences with other gesture-based interfaces. An important part of this is knowledge about the Kinect camera and its possibilities. The test users are worried of doing something that the application does not know how to process. A common problem is that it is unknown to the users that the Kinect can make a distinction between an open and a closed hand, even while there is visual feedback provided when opening or closing a hand: the hands on-screen turn respectively green or red. This means that the user does not consider opening or closing his hands when trying to interact with something on the screen. This problem is always related to the features supported by the used sensors. The Kinect camera supports this feature, but similar cameras may not. Because of this, it is not possible to simply assume user knowledge of this operation.\\

When test persons get the time to experiment with the interface at their own pace, they eventually find a way by themselves to interact with the application. Due to the limited amount of different interactive elements, the application has a gentle learning curve. A tutorial can help bringing new users up to speed with how to use the application. However, a tutorial can also be a source of frustration or confusion for the user if not done well. As such, tutorials should be seen more like a last resort instead of the go-to solution for any interface-related problem. More and better feedback and feedforward is generally the preferred option to choose, as it increases natural coupling between the required actions and its function. Also, providing redundancy concerning feedforward for an action can help making the interaction more clear as well. For instance, the color of an element changes when the users hovers over that element, implying that interaction is possible. This can be expanded by having a sound effect play at the same time the color changes.\\

A potential problem is the limited amount of space the user has in the room he is using the application. Unless the user is required to move left or right during the use of the application, all interactive elements must be placed within reach of the user's avatar. This means that non-interactive elements can be placed further away from the avatar. This introduces the problem of limited interface space on the screen and can result in elements that are functionally coupled together to be separated. For instance, the scrollbar with recorded gestures is on the right of the user's avatar, but the playback of the gesture is shown on the left. It limits the locational aspect of the functional feedback, but at the same time indicates that no specific action is required for interacting with the gesture display on the left as it is out of reach. For instance, to some extent, this can be compared to having to interact with a Blu-ray player, but receiving visual feedback from another place, i.e. the television screen.\\

Unrelated to the interface design, but an important part of the application is gesture recognition. Due to the chosen approach, it is possible to record and train exercises as gestures as well as postures. However, the way SVM works limits the requirement for high accuracy when executing a gesture or a posture. The executed exercise is always recognized as the recorded exercise that looks the most similar. If all of the recorded exercises are very dissimilar, exercises do not need to be accurately executed in order for them to be recognized as a specific exercise. This means that it is not possible to tell if an exercise is executed correctly. It requires the physical therapist to keep a close look at the way the patient performs an exercise during play.\\

Furthermore, the speed with which an exercise is executed is not checked. Only if the execution is too slow, it is not recognized as a recorded exercise. Faster execution does not affect the gesture recognition, on condition that the Kinect's sampling limitations are not surpassed. To solve this problem, as well as the previous problem of inaccurate executions, the therapist needs to record not only the required exercises, but also examples of how exercise should not be executed, i.e. exercises that are performed too slow, too fast or just not the way the gesture has to be done. This requires the therapist to record many examples of wrong executions, which requires a lot of time to do and eventually leads to a less pleasant experience during play as far fewer executions are accepted as being correctly executed. In addition, it requires the therapist to have some understanding of how machine learning works, which does not match the goal of designing an application that can be used without any programming knowledge.

\section{Reflection on the process}

There are different approaches for designing an interface, but it always comes down to not treating the interface as an afterthought. As such, integrating the interface design right from the start with the back-end software design is essential. For instance, requirements concerning the responsiveness of the interface are for a big part set by the way the back-end software is designed and implemented. In other words, if the back-end is programmed inefficiently, this is reflected by a lack of responsiveness in the GUI.\\

However, it is necessary to switch fast from interface prototyping to an actual implementation. Since the early prototypes are static and non-interactive, it is only possible to really evaluate a decision after having an implementation of the concept and to obtain feedback from the users it is intended for. In other words, while static prototypes can undergo several iterations until they are \emph{perfected} in theory, only implementations in practice can really convey the feel of the interface.\\

To come up with ways of interacting with the application other than relying on traditional controls, an experimental approach is tested. The idea is to interview a physical therapist in order to obtain ideas from him concerning the interaction with the application. This makes it possible to find solutions that fit the needs and expectations of the user. Early tryouts of this experimental technique show that persons are reluctant of coming up with ideas they are not familiar with. It is more assuring and easier to fall back on traditional, well-known elements.\\

In the same sense, user testing often leads to results implying that more conventional interactive elements are easier and quicker to understand than others, even if they rely on making use of the mime pattern discussed before. This can be related to years of experience with other types of interfaces and confirms that there is need of a more streamlined way for gesture-based interaction that unifies all interfaces of this kind \cite{Norman2010}.\\

Including a scrollbar in a gesture-based interface is an example of this. The user has experience using scrollbars, so there are no problems to recognize the visual element as a list that can be scrolled through, but interacting with it is more difficult initially. This can be related to differences in the way gesture-based and touchscreen-based interfaces work. On touchscreens, it is possible to swipe with a finger on the screen to scroll, then lift it and touch the screen again to scroll again. With gesture-based interfaces, the action analogous to lifting the finger is less obvious to achieve. One way is to have the user move his hand out of the scrollbar before he can scroll in the same direction again. This is how it is implemented for the application. A different approach is to use composite gestures. The user can close his hand to grab the scrollbar and scroll through it, then open his hand, reposition it and close it to scroll again. This allows the action of scrolling to be faster, but it also adds the burden of having to remember more actions for interacting with this element and it can be tiresome to close and reopen the hand many times in succession. In the end, this comes down to personal preference and requires more user tests before a more general conclusion can be made.\\


\section{Future work}

Physical therapist Dries Lamberts indicated that the action of recording and training gestures can be pleasant for patients to do it themselves. This is especially useful if the developed application is not exclusively used for rehabilitation, but also as a way to have some physical exercise on a regular basis, as is the case for children with physical disabilities. As the GUI of the application is focused on being used by therapists, a different interface is required to simplify and optimize its use for children. This includes features like pressing a keyboard key to be designed appropriately.\\

A second possibility for future work is to use the current back-end structure to allow the user to create patient-specific projects, to which all exercises are coupled. On startup of the application, it can be possible to require the user to enter the name of a patient to load the project with all exercises they used before. This decreases the time needed before a patient can start exercising and playing a game.\\

Finally, the application can be expanded by adding screens to the interface that allow the user to manage different gesture classes and projects, reusing assets that are present in the developed interface.
